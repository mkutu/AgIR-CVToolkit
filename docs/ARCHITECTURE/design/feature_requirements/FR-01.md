# FR-01 · Query the AgIR SemiF DB (SQLite) and return normalized records

**Goal:** Provide a single, well-typed entry point to query the SemiF SQLite DB and get back rows in a stable, schema-checked structure that downstream code (downloaders, samplers, exporters) can rely on.

## Scope

* Input: structured filters (species, estimated_bboxcm2, location, date ranges, splits, has_mask, etc.), projection (which columns), sort, limit/offset.
* Output: typed rows (Pydantic/dataclass) with **resolved file pointers** (absolute POSIX paths), plus minimal relational expansions (e.g., join to categories/annotations when requested).

## Non-Goals (for FR-01)

* No downloading or file I/O beyond **path resolution**.
* No schema migrations (read-only).
* No network storage logic (handled in FR-02).

## Interfaces

* **Python API**

  * `AgirDBClient(sqlite_path: Path, root_map: dict[str, Path])`
  * `query(self, q: QuerySpec) -> Iterator[ImageRecord]`
  * `preview(self, q: QuerySpec, n: int = 10) -> list[ImageRecord]`

* **Types**

  * `QuerySpec`

    * `filters: dict[str, Any]` (supports scalars, lists, ranges)
    * `projection: list[str] | None`
    * `sort: list[tuple[str, Literal["asc","desc"]]] | None`
    * `limit: int | None`, `offset: int | None`
    * `expand: dict[str, bool]` (e.g., `{"category": true, "annotations": false}`)
  * `ImageRecord` (dataclass)

    * core: `image_id`, `batch_id`, `timestamp`, `camera`, `width`, `height`
    * paths: `image_path: Path`, `mask_path: Path | None`, `aux_paths: dict[str, Path]`
    * labels/meta: `species`, `growth_stage | None`, `split | None`, `geom | None`
    * hashes: `sha256 | None`, `size_bytes | None`

## Config (Hydra)

```yaml
db:
  sqlite_path: ${paths.sqlite}/agir_field.db
  # map dataset logical roots -> mounted filesystem roots
  root_map:
    LTS: /mnt/research-projects/agir_lts
    STAGING: /mnt/staging
query_defaults:
  limit: 1000
  expand:
    category: true
    annotations: false
```

## CLI (Typer)

* `agir-cv query --filters 'species==["vetch","barley"]' --filters 'has_mask==true' --limit 200 --expand category=true`
* Flags:

  * `--filters` (repeatable; Python-like mini-DSL or JSON)
  * `--projection`, `--sort`, `--limit`, `--offset`, `--expand`
  * `--out csv|json|parquet` + `--out-path`

## Behavior & Rules

* **Filter parsing:**

  * Scalars: `key==value` → `= ?`
  * Lists: `key==[a,b]` → `IN (?,?)`
  * Ranges: `date>=2024-01-01`, `area in [10,100]` → `BETWEEN ? AND ?`
  * Nullability: `mask_path is null` / `is not null`
* **Root resolution:** If a DB path starts with a logical root (`LTS:/foo/bar.jpg`), convert via `root_map` to absolute `Path`.
* **Projections:** Only include requested columns; always include primary identifiers.
* **Paging:** Support `limit`/`offset` deterministically with a default sort key (e.g., `image_id asc`) if none provided.
* **Validation:** Rows are validated into `ImageRecord`; invalid rows are logged at `WARNING` with row id and skipped (configurable `fail_on_invalid`).

## Logging

* Format: `[%(asctime)s][%(name)s][%(levelname)s] - %(message)s`
* Log the **final** SQL, **bound params**, and **row count**.

## Telemetry / Counters

* `rows_scanned`, `rows_returned`, `rows_invalid`, `query_ms`.

## Edge Cases

* Broken path roots → raise `ConfigurationError` with the missing root name.
* Inconsistent path separators → normalize to POSIX internally.
* Giant `IN` clauses → chunk into ≤1000 params per clause.

## Definition of Done (DoD)

* Unit tests: filter parsing, path resolution, expansions, paging, validation.
* Integration test: point at a small sample DB; deterministic snapshot tests for a known query.
* CLI smoke tests: `--out csv|json` produce identical content to API call.
* Performance: 100k-row query completes and streams within memory bounds (iterator-based).

---

# FR-02 · Fetch/Stage Files Referenced by Query Results

**Goal:** Given `ImageRecord`s (from FR-01), pull the referenced assets (images, masks, aux) from local/NFS to a **target workspace** with optional compression/resize, integrity checks, and a reproducible manifest.

## Scope

* Inputs: iterator of `ImageRecord`, `FetchPlan` describing what to pull and how to transform.
* Outputs: files materialized under `workspace_root`, a manifest (`manifest.csv` / `manifest.parquet`), and a **content-addressed cache** to avoid re-downloads.

## Non-Goals (for FR-02)

* Remote protocols beyond local/NFS (S3/Globus can be future **FR-02b**).
* Label format conversions (handled later).

## Interfaces

* **Python API**

  * `Fetcher(workspace_root: Path, cache_root: Path | None = None, workers: int = 4)`
  * `stage(records: Iterable[ImageRecord], plan: FetchPlan) -> StageReport`

* **Types**

  * `FetchPlan`

    * `include: {"image": true, "mask": true, "aux": ["depth","ndvi"]}`
    * `layout: "flat" | "by_batch" | "by_species"` (foldering scheme)
    * `transform:`

      * `resize: {max_side: int} | null` (optional; preserve aspect)
      * `compress: {format: "jpg"|"png", quality: int} | null` (if no resize: **re-encode only**)
      * `strip_exif: bool`
    * `checks:` `{sha256: bool, size: bool}`
    * `retry:` `{max_retries: 2, backoff: 0.5}`
  * `StageReport`

    * counts: `requested`, `copied`, `skipped_cached`, `failed`
    * timings, bytes_moved, manifest_path

## Config (Hydra)

```yaml
io:
  workspace_root: ${paths.work}/staged
  cache_root: ${paths/work}/cache
fetch:
  workers: 8
  default_plan:
    include: {image: true, mask: true, aux: []}
    layout: by_batch
    transform:
      resize: null           # or {max_side: 2048}
      compress: {format: jpg, quality: 88}
      strip_exif: true
    checks: {sha256: true, size: true}
    retry: {max_retries: 2, backoff: 0.5}
```

## CLI (Typer)

* **Two-step happy path**

  1. `agir-cv query ... --out json --out-path q.json`
  2. `agir-cv fetch --from q.json --layout by_batch --compress jpg:85 --strip-exif`
* Direct pipe path: `agir-cv query ... | agir-cv fetch --stdin`

### CLI flags (fetch)

* `--from PATH | --stdin`
* `--include image,mask,aux:depth`
* `--layout flat|by_batch|by_species`
* `--resize 2048` (max side), or omit to keep native size
* `--compress jpg:85 | png:6 | none`
* `--strip-exif / --keep-exif`
* `--workers 8`
* `--manifest manifest.parquet`

## Behavior & Rules

* **Idempotence & Caching:**

  * Destination path is a deterministic function of `(record.image_id, variant)`.
  * If file already exists **and** hash matches (or size when hash unavailable), mark `skipped_cached`.
* **Transforms:**

  * If `resize` is null and `compress` is set → **re-encode without resizing** (your “don’t resize, just compress” case).
  * PNG masks: enforce lossless if mask data; images can be JPEG by plan.
* **Integrity:**

  * If `checks.sha256`, verify source hash if present; else compute on destination post-copy.
  * If mismatch: retry with backoff; on repeat failure, log `ERROR` and continue.
* **Parallelism:** worker pool with bounded queues; soft-fail per item to keep the pipeline moving.
* **Manifest:** write one row per materialized artifact with columns: `image_id`, `kind (image|mask|aux)`, `src_path`, `dst_path`, `width`, `height`, `orig_bytes`, `dst_bytes`, `sha256`, `status`.

## Logging

* Each file op at `INFO` (src→dst + ops), retries at `WARNING`, failures at `ERROR`.
* Final summary with counts and throughput (MiB/s).

## Edge Cases

* Missing `mask_path` when `include.mask=true` → record as `skipped_missing`.
* Permission errors on NFS → `ERROR` with errno, keep going.
* Oversized images (>32k px): fallback to tiled re-encode if pillow/cv2 balks; emit `WARNING`.

## Definition of Done (DoD)

* Unit tests: destination layout, idempotence, transform paths, EXIF strip, hash checks.
* Integration test: stage a 50-record sample; verify manifest, byte deltas with/without compression.
* Performance: With `workers=8`, at least 100 MB/s aggregate on local/NFS (hardware permitting).
* Determinism: same input → same destination paths and manifest rows.

---

## How FR-01 and FR-02 compose (happy path)

1. `AgirDBClient.query(QuerySpec)` → stream `ImageRecord`s
2. `Fetcher.stage(records, plan)` → files + `manifest.parquet`
3. Downstream steps (future FRs) consume the manifest (e.g., CVAT uploader, training datamodule builder)

---

## Immediate Next Actions (so you can start coding confidently)

1. **Create modules & types**

   * `src/agir_cvtoolkit/db/client.py` (FR-01)
   * `src/agir_cvtoolkit/db/types.py` (`QuerySpec`, `ImageRecord`)
   * `src/agir_cvtoolkit/fetcher/fetcher.py` (FR-02)
   * `src/agir_cvtoolkit/fetcher/types.py` (`FetchPlan`, `StageReport`)

2. **Hydra config stubs**

   * `conf/config.yaml` with `db`, `io`, `fetch`, and your logging format.
   * `conf/query/dev.yaml` sample filters for quick manual tests.

3. **CLI commands**

   * `agir-cv query ...`
   * `agir-cv fetch ...`

4. **Tests (first pass)**

   * Tiny fixture DB (10 rows) + tiny images/masks in `tests/data`.
   * Snapshot test for a canonical `QuerySpec`.
   * Fetch test that proves: compress-only works, idempotence works, manifest correctness.

